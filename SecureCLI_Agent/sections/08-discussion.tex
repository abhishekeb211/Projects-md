% Section 8: Discussion
% Converted from 4.8-discussion.md
% Module 5.2: Section Integration & Flow Refinement

\subsection{RQ Answers}

Our evaluation addresses five research questions (RQ1--RQ5) through four controlled experiments (E1--E4) and one qualitative study (RQ5), providing evidence-based answers to each research question:

\textbf{RQ1 (Orchestration Efficiency):} Does \sarif{}-based scanner orchestration reduce tool-switching overhead compared to manual multi-tool workflows, and by what magnitude? \textbf{Answer:} AegisCLI reduces tool-switching overhead by 62\% (baseline: 8.3 min $\to$ AegisCLI: 3.2 min, $p<0.001$), supporting H1 and validating unified orchestration as an effective solution for tool sprawl. This reduction is achieved through \sarif{} normalization (eliminating manual reconciliation), unified CLI interface (single-command scan), and cross-tool deduplication (preventing redundant findings). The magnitude of reduction (62\%) exceeds our hypothesis threshold ($\geq$60\%), indicating that orchestration benefits are substantial and practically meaningful.

\textbf{RQ2 (\llm{} Triage Accuracy):} Can local CodeLlama 13B achieve acceptable triage accuracy ($\kappa \geq 0.75$) compared to expert security panels, validating privacy-preserving AI as a viable alternative to cloud-based \llm s? \textbf{Answer:} CodeLlama 13B achieves $\kappa = 0.78$ (95\% CI [0.71, 0.84]) compared to expert consensus, validating H2 and demonstrating that local-first \llm{} deployment provides acceptable accuracy for privacy-sensitive contexts. While CodeLlama achieves slightly lower accuracy than cloud-based GPT-4 ($\kappa = 0.82$), the 4-point difference (78\% vs. 82\%) is within acceptable tradeoff range for organizations prioritizing privacy compliance over marginal accuracy gains. This result validates the privacy-performance tradeoff identified in Section~1.3, enabling regulated industries to adopt AI-powered security automation without violating data residency requirements.

\textbf{RQ3 (Policy-as-Code Effectiveness):} Does policy-as-code enforcement reduce security debt accumulation velocity compared to manual policy enforcement, measured as quarterly debt velocity (issues per quarter)? \textbf{Answer:} \pac{}-enabled teams reduce debt accumulation by 43.1\% (non-\pac{}: +12.3 issues/quarter $\to$ \pac{}: +7.0 issues/quarter, $p<0.001$), supporting H3 and validating policy automation as an effective debt management strategy. This reduction exceeds our hypothesis threshold ($\geq$40\%), indicating that policy automation benefits are substantial and organizationally meaningful. The debt velocity reduction is achieved through automated remediation SLAs (critical findings must be remediated within 48 hours), exemption rules (champion-approved exceptions), and prioritization policies (severity-weighted debt tracking).

\textbf{RQ4 (Champion Program Impact):} Do teams with designated security champions exhibit faster \mttr{} than teams without champions, and what factors mediate this relationship? \textbf{Answer:} Champion-enabled teams achieve 28\% faster \mttr{} (champion teams: 14.2 hours $\to$ non-champion teams: 22.8 hours, $p<0.001$), supporting H4 and validating the security champion model as an effective adoption enabler. The magnitude of improvement (28\%) exceeds our hypothesis threshold ($\geq$25\%), indicating that champion programs provide substantial organizational benefits. Regression analysis reveals that champion effectiveness correlates with technical expertise (security-certified champions show stronger impact than general-engineering champions), team maturity (mature teams with existing security practices benefit more from champions), and active engagement (champions with $\geq$1 activity per month show stronger impact than passive champions).

\textbf{RQ5 (Adoption Barriers):} What adoption barriers and enablers emerge when deploying agentic security tools in production, and how do organizational factors (team size, maturity, regulatory requirements) influence adoption success? \textbf{Answer:} Qualitative analysis (10 champion interviews, thematic analysis) identifies three primary adoption barriers---trust in \llm{} decisions (developers question ``black box'' triage classifications), workflow integration friction (policy configuration complexity, CLI learning curve), and organizational resistance (budget constraints, tool fatigue)---and three primary enablers---champion advocacy (champions address developer trust concerns, provide training), developer training (workshops on \llm{} triage, policy configuration tutorials), and organizational support (executive buy-in, security culture alignment). Organizational factors mediate adoption success: mature teams with existing security practices adopt faster than immature teams (adoption time: 2 weeks vs. 6 weeks), teams with regulatory requirements (GDPR, HIPAA) prioritize privacy-preserving features (local \llm{} deployment), and large teams ($>30$ engineers) benefit more from unified orchestration than small teams ($<10$ engineers) due to scale effects.

\subsection{ST-SSDLC Implications}

Our evaluation extends the Socio-Technical Secure Software Development Lifecycle (ST-SSDLC) framework (Farnsworth 2021) by incorporating local-first AI as a privacy-preserving automation mechanism, demonstrating how organizational privacy constraints can be reconciled with AI-powered security workflows. This extension provides a conceptual model for future research on air-gapped DevSecOps automation and informs practitioner decisions about AI tool adoption under regulatory constraints.

\textbf{Local-First AI as Socio-Technical Enabler.} Local-first AI deployment (CodeLlama 13B via Ollama) addresses the ST-SSDLC organizational dimension by enabling privacy-preserving automation in regulated industries (defense, healthcare, finance) that cannot adopt cloud-based AI tools. Our evaluation demonstrates that local \llm{} triage achieves acceptable accuracy ($\kappa = 0.78$) while preserving privacy compliance (no code exfiltration, air-gap deployment), validating the privacy-performance tradeoff as a viable organizational strategy. This result extends ST-SSDLC theory by identifying local-first AI as a socio-technical enabler that reconciles privacy constraints with automation benefits, addressing the organizational resistance barrier identified in RQ5.

\textbf{Champion-AI Tool Interaction.} Our evaluation reveals that security champions play a critical role in facilitating AI tool adoption, addressing the trust barrier identified in RQ5 (developers question ``black box'' \llm{} decisions). Champions provide trust calibration (explaining \llm{} classifications, handling edge cases), workflow integration support (policy configuration tutorials, CLI training), and organizational advocacy (executive buy-in, security culture alignment). This champion-AI tool interaction extends ST-SSDLC theory by demonstrating how organizational factors (champion programs) mediate technical factors (AI tool effectiveness), providing a conceptual model for future research on human-AI collaboration in security automation.

\textbf{Privacy-by-Design as Organizational Practice.} AegisCLI's privacy-by-design architecture (air-gap mode, secret redaction, telemetry minimization) demonstrates how technical privacy safeguards can be embedded in organizational security practices, addressing the regulatory compliance barrier identified in RQ5. Our evaluation validates that privacy-by-design enables organizations with strict data residency requirements (GDPR, HIPAA, PCI-DSS) to adopt AI-powered security automation without violating regulatory constraints, extending ST-SSDLC theory by identifying privacy-by-design as an organizational practice that reconciles compliance requirements with automation benefits.

\subsection{Industrial Adoption}

Our evaluation provides evidence-based guidance on industrial adoption strategies for agentic security tools, including phased rollout lessons, champion enablement playbooks, and organizational readiness assessment. These adoption strategies address the workflow integration friction and organizational resistance barriers identified in RQ5, enabling practitioners to deploy AegisCLI and similar tools in production environments.

\textbf{Phased Rollout Lessons.} Our 12-month phased deployment (P0--P4) reveals three critical lessons: (1) \textbf{Pilot Deployment (P2)} is essential for identifying adoption barriers (trust, friction, resistance) before full-scale deployment, enabling refinement of training materials, policy templates, and workflow integration strategies. (2) \textbf{Champion Enablement (P2--P4)} accelerates adoption by addressing developer trust concerns (\llm{} decision explanations), providing workflow integration support (policy configuration tutorials), and facilitating organizational advocacy (executive buy-in, security culture alignment). (3) \textbf{Performance Optimization (P3)} is critical for production scale (Redis caching for policy decisions reduces latency by 80\%, dependency caching for \llm{} inference reduces latency by 80\%), ensuring that tool adoption does not introduce performance bottlenecks.

\textbf{Champion Enablement Playbook.} Our evaluation identifies a champion enablement playbook for AI tool adoption: (1) \textbf{Champion Selection Criteria}: Security-certified champions with 3+ years experience show stronger impact than general-engineering champions, indicating that technical expertise matters for champion effectiveness. (2) \textbf{Active Engagement Metrics}: Champions with $\geq$1 security activity per month (policy review, training, incident response) show stronger impact than passive champions, indicating that engagement matters for champion effectiveness. (3) \textbf{Trust Calibration Strategies}: Champions address developer trust concerns by explaining \llm{} classifications (confidence scores, explanations), handling edge cases (low-confidence findings flagged for human review), and providing training workshops (\llm{} triage tutorials, policy configuration guides).

\textbf{Organizational Readiness Assessment.} Our evaluation reveals that organizational readiness factors mediate adoption success: (1) \textbf{Team Maturity}: Mature teams with existing security practices (security maturity score $\geq$3/5) adopt faster than immature teams (adoption time: 2 weeks vs. 6 weeks), indicating that organizational maturity matters for adoption success. (2) \textbf{Regulatory Requirements}: Teams with strict data residency requirements (GDPR, HIPAA, PCI-DSS) prioritize privacy-preserving features (local \llm{} deployment), indicating that regulatory context matters for adoption priorities. (3) \textbf{Team Size}: Large teams ($>30$ engineers) benefit more from unified orchestration than small teams ($<10$ engineers) due to scale effects (tool-switching overhead scales non-linearly with tool count), indicating that organizational scale matters for adoption benefits.

% Transition paragraph to Section 9 (Limitations)
Having discussed the research contributions and industrial adoption implications, the next section acknowledges limitations of our evaluation, addressing threats to validity and proposing future work to address these boundaries.

