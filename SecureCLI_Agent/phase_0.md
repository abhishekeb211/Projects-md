# PHASE 0: Project Initialization & Research Ledger Setup

## Module 0.1: Research Ledger Foundation - Output

### Research Ledger v0 (ledger-v0.md)

# AegisCLI Research Ledger v0
**Version:** 0.1  
**Last Updated:** Week 0  
**Status:** Initial Setup

---

## 1. Definitions

### MTTR (Mean Time To Remediate)
- **Definition:** Average time from when a security finding is detected to when it is fully remediated and verified.
- **Measurement Unit:** Hours (or days for long-running issues)
- **Formula:** `MTTR = Î£(remediation_time - detection_time) / n_findings`
- **Baseline Target:** Measure pre-AegisCLI MTTR from 50-repo dataset

### Security Debt
- **Definition:** Accumulated unresolved security findings that represent technical debt requiring future remediation effort.
- **Measurement Unit:** Count of unresolved findings weighted by severity (Critical=4, High=3, Medium=2, Low=1)
- **Formula:** `Security Debt = Î£(finding_count Ã— severity_weight)`
- **Tracking Method:** Quarterly snapshots comparing debt velocity (rate of accumulation vs. remediation)

### Tool Sprawl
- **Definition:** The proliferation of multiple disconnected security scanning tools requiring manual context-switching by developers.
- **Measurement Unit:** 
  - Count of distinct tools per team
  - Average context-switch time (minutes) between tools
  - Overhead percentage = (tool_switch_time / total_scan_time) Ã— 100
- **Baseline Metric:** From 50-repo survey, measure average tools per team and switch overhead

### Agentic AI
- **Definition:** AI system that autonomously orchestrates multiple actions (scanner invocation, result normalization, triage decision-making) within defined policy boundaries.
- **Key Characteristics:** 
  - Autonomy level: Orchestration (not autonomous code changes)
  - Decision boundaries: Policy-as-Code rules + confidence thresholds
  - Human-in-the-loop triggers: Low confidence (<0.8) or policy violations

---

## 2. Baselines

### Pre-AegisCLI Scan Times
- **Data Source:** CI/CD logs from 50 target repositories (prior to AegisCLI integration)
- **Collection Period:** Last 6 months (if available) or next 2 weeks during P0
- **Metrics:**
  - Average scan duration per repository: `[TBD - collect during P0]`
  - Tool switch overhead time: `[TBD - measured via developer surveys]`
  - Total pipeline time including security scans: `[TBD]`

### False Positive Rates from 50 Repos
- **Data Source:** Manual annotation of findings from 50 repos baseline scan
- **Collection Method:** Stratified sample (50 findings per severity level = 200 total)
- **Metrics:**
  - Overall FP rate: `[TBD - target baseline <30%]`
  - FP rate by scanner: `semgrep=[TBD], trivy=[TBD], checkov=[TBD]`
  - FP rate by severity: `CRITICAL=[TBD], HIGH=[TBD], MEDIUM=[TBD], LOW=[TBD]`
- **Gold Standard:** 3 senior security engineers, independent labeling, consensus via majority vote

---

## 3. Assumptions

### CodeLlama Accuracy
- **Assumption:** CodeLlama 13B (via Ollama) achieves Îº â‰¥ 0.75 inter-annotator agreement with expert security panel for severity triage.
- **Rationale:** Based on preliminary tests on 20 sample findings (informal, not yet validated)
- **Validation Method:** Module 3.4 Experiment E2 with 200 stratified findings
- **Risk if False:** Fallback to GPT-4 API (privacy tradeoff) or manual triage only

### Policy-as-Code (PaC) Effectiveness
- **Assumption:** OPA/Rego policies reduce security debt accumulation by â‰¥40% compared to manual policy enforcement.
- **Rationale:** DORA metrics show automation typically improves MTTR by 30-50%
- **Validation Method:** Module 3.4 Experiment E3 (A/B: PaC-enabled vs. manual teams)
- **Risk if False:** Re-evaluate PaC rule complexity; consider hybrid manual+automated approach

### Champion Effect
- **Assumption:** Teams with designated security champions show â‰¥25% faster MTTR than teams without champions.
- **Rationale:** BSIMM studies show champion programs improve security culture
- **Validation Method:** Module 3.4 Experiment E4 (correlate champion presence with MTTR)
- **Risk if False:** Champion identification criteria may need refinement; effect may be confounded with team maturity

---

## 4. Decisions Log

| Date | Decision | Rationale | Impact | Owner |
|------|----------|-----------|--------|-------|
| Week 0 | Use SARIF v2.1.0 as normalization schema | Industry standard, tool ecosystem support | Enables scanner agnosticism | Engineering Lead |
| Week 0 | CodeLlama 13B over GPT-4 for triage | Privacy (air-gap requirement), cost, reproducibility | Local deployment complexity; may need GPU | Research Lead |
| Week 0 | OPA/Rego for Policy-as-Code | Maturity, performance, open-source | Team must learn Rego syntax | Engineering Lead |
| Week 0 | Go language for CLI core | Concurrency, static binaries (air-gap), performance | Faster development; easier deployment | Engineering Lead |
| Week 0 | 50 repos as baseline dataset | Statistical power (50 repos Ã— 20 commits = 1000 points) | Requires access agreements from 50 repo owners | Research Lead |
| Week 0 | Ollama as LLM runtime | Local-first, no cloud dependencies | Must pre-download models for air-gap | Engineering Lead |
| Week 0 | PostgreSQL + Grafana for dashboard | Open-source, SQL queries for metrics | Requires DB infrastructure setup | Engineering Lead |

---

## 5. Locked Parameters

### Scope
- **Geographic:** Single organization (large-scale, Microsoft-like)
- **Languages:** 5 ecosystems (Node.js, Python, Go, Java, Terraform/IaC)
- **Time Horizon:** 12-month longitudinal study (P0-P4 phases)
- **Team Size:** 500+ engineers, 20+ teams
- **Repository Count:** 50+ repos for baseline, 10+ repos for deep evaluation

### LLM Model
- **Primary:** CodeLlama 13B (quantized, via Ollama)
- **Fallback:** GPT-4 API (only if CodeLlama Îº < 0.70, with privacy waiver)
- **Quantization:** Q4_K_M (balance of size vs. accuracy)
- **Context Window:** 4096 tokens (5-shot prompts + finding context)

### Normalization Schema
- **Standard:** SARIF v2.1.0 (OASIS standard)
- **Subset:** Full result objects, rule metadata, location (file/line), severity
- **Extensions:** Custom properties for AegisCLI metadata (confidence, policy_decision, champion_review_flag)

---

## 6. Dependencies

### Scanner Versions
- **semgrep:** v1.45.0+ (SARIF output support)
- **trivy:** v0.48.0+ (SARIF v2.1.0)
- **checkov:** v3.0.0+ (SARIF support)
- **gitleaks:** v8.18.0+ (for secret redaction before LLM context)
- **Note:** Test compatibility with latest versions quarterly; pin versions for reproducibility

### Ollama Version
- **Required:** v0.1.15+ (CodeLlama 13B support)
- **Model:** `codellama:13b` (pinned via `ollama pull codellama:13b`)
- **Storage:** ~7GB per model (Q4_K_M quantization)

### Go Version
- **Required:** Go 1.21+ (for CLI core, scanner adapters)
- **Rationale:** Static binary generation, cross-platform support
- **Build Target:** Linux amd64, Windows amd64 (for air-gap deployments)

### Other Dependencies
- **OPA:** v0.60.0+ (Rego policy engine)
- **Redis:** v7.0+ (optional, for policy decision caching)
- **PostgreSQL:** v15+ (for dashboard/metrics DB)

---

## 7. Placeholders & Future Updates

### 90-Day Rolling Telemetry Policy
- **Status:** [Placeholder - to be defined in Week 2]
- **Policy Document:** `docs/telemetry-policy.md`
- **Requirements:** Opt-in consent, data retention limits, anonymization rules

### IRB Consent Form References
- **Status:** [Placeholder - IRB submission pending]
- **Document ID:** [TBD after IRB approval]
- **Consent Scope:** Interview participants (champions), telemetry opt-in for teams

---

## Module 0.2: Research Scope & Constraint Mapping - Output

### Constraints Mapping Document (constraints-mapping.md)

# Research Scope & Constraint Mapping

**Version:** 1.0  
**Date:** Week 0  
**Status:** Initial Mapping

---

## 1. Code Size Constraints (50K+ LOC)

### Language Breakdown Targets

| Language/Ecosystem | Target LOC | Rationale | Priority |
|-------------------|------------|-----------|----------|
| Node.js/TypeScript | 15,000 LOC | High adoption, npm ecosystem vulnerabilities | P0 |
| Python | 12,000 LOC | Common in ML/data teams, dependency issues | P0 |
| Go | 10,000 LOC | Infrastructure code, static analysis needs | P1 |
| Java | 8,000 LOC | Enterprise codebases, Maven/Gradle scans | P1 |
| Terraform/IaC | 5,000 LOC | Infrastructure-as-Code security critical | P0 |

**Total Target:** 50,000 LOC minimum  
**Measurement Method:** `cloc` or `tokei` tool per repository  
**Validation:** Run LOC counts on 50 baseline repos; ensure â‰¥50K total

---

## 2. Scan Run Targets (10,000+ Scan Runs)

### Daily/Weekly Scan Quotas

| Team Category | Teams | Scans/Week per Team | Total Scans/Week | Annual Target |
|--------------|-------|---------------------|------------------|---------------|
| High-activity teams (P0 repos) | 5 | 40 scans | 200 | 10,400 |
| Medium-activity teams | 10 | 20 scans | 200 | 10,400 |
| Low-activity teams | 5 | 8 scans | 40 | 2,080 |
| **Total** | **20** | **Varies** | **440/week** | **22,880/year** |

**Note:** 10,000 scans = ~6 months at 440 scans/week, or 1 year at 192 scans/week

### Distribution Assumptions
- **Per-repo frequency:** Average 4 scans/week (daily CI + manual)
- **Peak periods:** Sprint ends may see 2Ã— frequency
- **Holiday/lull periods:** 0.5Ã— frequency acceptable

**Validation Method:** Monitor CI/CD integration logs; track scan invocation counts via AegisCLI telemetry

---

## 3. Air-Gapped Environment Technical Specs

### Network Constraints
- **Outbound Network:** Blocked (no external API calls after initial setup)
- **Inbound Network:** Blocked (no cloud services, no telemetry uploads)
- **Local Network:** Allowed (within organization firewall)
- **USB Transfer:** Allowed (for model distribution, artifact updates)

### Ollama Local Model Cache Requirements
- **Model Storage:** ~7GB per model (CodeLlama 13B Q4_K_M)
- **Cache Location:** `/var/lib/ollama/models/` (configurable via `OLLAMA_MODELS` env var)
- **Pre-deployment:** Models must be downloaded and transferred via USB/internal network before air-gap activation
- **Update Mechanism:** Manual USB transfer of new model versions (quarterly updates expected)

### Deployment Specifications
- **Installation Method:** Static Go binaries + Ollama binary (no internet required post-install)
- **Configuration:** YAML config files (no cloud-based config services)
- **Database:** Local PostgreSQL instance (no cloud DB connections)
- **Artifact Distribution:** Via internal artifact repository or USB drives

**Validation Test:** Deploy AegisCLI on isolated Ubuntu 22.04 VM (network disabled) and verify full functionality

---

## 4. Critical Feasibility Risks

### Risk 1: GPU Availability for CodeLlama 13B

**Probability:** Medium (60%)  
**Impact:** High (blocks LLM triage feature)  
**Risk Score:** 18/25 (High Risk)

**Description:**
- CodeLlama 13B requires GPU (8GB+ VRAM) for acceptable inference speed (<5s per finding)
- Air-gapped environments may not have GPU infrastructure
- CPU-only inference is 10-20Ã— slower, making real-time triage impractical

**Mitigation Strategy:**
- **Primary:** Pre-validate GPU availability on target air-gap systems (Week 1)
- **Backup Plan 1:** Use quantized smaller model (CodeLlama 7B) if GPU unavailable (accuracy tradeoff: Îº may drop to 0.70)
- **Backup Plan 2:** Batch processing (queue findings, process overnight) if CPU-only required
- **Backup Plan 3:** Hybrid approach: Critical findings â†’ GPT-4 API (with privacy waiver), others â†’ CPU CodeLlama

**Owner:** Engineering Lead  
**Timeline:** GPU validation by Week 2; fallback decision by Week 3

---

### Risk 2: Scanner API Churn

**Probability:** Medium (50%)  
**Impact:** Medium (requires adapter updates)  
**Risk Score:** 12.5/25 (Medium Risk)

**Description:**
- Scanner tools (semgrep, trivy, checkov) update frequently; SARIF output format may change
- Breaking changes could break AegisCLI scanner adapters
- May cause delays in research timeline if adapters need rewriting

**Mitigation Strategy:**
- **Primary:** Pin scanner versions in baseline (semgrep v1.45.0, trivy v0.48.0, checkov v3.0.0)
- **Backup Plan 1:** Version compatibility matrix (test new versions quarterly, update adapters if needed)
- **Backup Plan 2:** SARIF schema validation (fail-fast if format unexpected)
- **Backup Plan 3:** Maintain fork/patched versions of scanners if upstream breaks compatibility

**Owner:** Engineering Lead  
**Timeline:** Version pinning in Week 1; quarterly compatibility tests

---

### Risk 3: Champion Attrition

**Probability:** Low (30%)  
**Impact:** High (reduces adoption data quality)  
**Risk Score:** 9/25 (Medium Risk)

**Description:**
- Security champions may leave organization, change roles, or lose engagement
- Champion effect (H4) requires consistent champion presence over 12 months
- Loss of champions in P2-P4 phases could reduce statistical power for E4 experiment

**Mitigation Strategy:**
- **Primary:** Identify backup champions (2 per team) at P0; rotate if primary champion unavailable
- **Backup Plan 1:** Champion training program (standardize knowledge, reduce dependency on individuals)
- **Backup Plan 2:** Adjust E4 design to use "champion-months" as exposure variable (handles partial participation)
- **Backup Plan 3:** Expand sample size by 20% (buffer for attrition) - recruit 24 teams instead of 20

**Owner:** Research Lead  
**Timeline:** Backup champion identification by Week 4; attrition monitoring monthly

---

## 5. Risk Matrix

| Risk ID | Risk Description | Probability | Impact | Risk Score | Mitigation Owner | Status |
|---------|------------------|-------------|--------|------------|------------------|--------|
| R1 | GPU Availability | Medium (60%) | High | 18/25 | Engineering Lead | Monitoring |
| R2 | Scanner API Churn | Medium (50%) | Medium | 12.5/25 | Engineering Lead | Mitigated (version pinning) |
| R3 | Champion Attrition | Low (30%) | High | 9/25 | Research Lead | Mitigated (backup champions) |

**Risk Score Calculation:** Probability (1-5 scale) Ã— Impact (1-5 scale) = Risk Score (1-25 scale)

**Risk Thresholds:**
- **Critical (20-25):** Immediate action required
- **High (12-19):** Mitigation plan active, monitor weekly
- **Medium (6-11):** Mitigation plan defined, monitor monthly
- **Low (1-5):** Acceptable risk, monitor quarterly

---

## 6. Additional Constraints & Boundaries

### Time Constraints
- **Phase 0-1 (Setup):** 4 weeks
- **Phase 2 (SLR):** 8 weeks
- **Phase 3-4 (Development & Evaluation):** 32 weeks
- **Phase 5-7 (Writing & Submission):** 16 weeks
- **Total Timeline:** 60 weeks (~15 months)

### Resource Constraints
- **Engineering Team:** 2-3 engineers part-time (50% allocation)
- **Research Team:** 2 researchers full-time
- **Infrastructure:** Shared (no dedicated GPU clusters; must use existing air-gap systems)

### Data Constraints
- **Privacy:** No code snippets >5 lines in telemetry (redaction required)
- **Consent:** IRB approval required for interviews; opt-in telemetry for teams
- **Retention:** 90-day rolling window for telemetry data (policy TBD)

---

## Summary

All constraints have been mapped to quantifiable metrics with validation methods. Three critical risks identified with mitigation strategies and assigned owners. Risk matrix shows R1 (GPU) as highest priority for monitoring.

---

## PHASE 0 TODO LIST & ACTIONABLE TASKS

This section lists all actionable tasks, commands, and deliverables that need to be executed to complete Phase 0.

### Module 0.1: Research Ledger Foundation

#### âœ… Completed Tasks
- [x] Research Ledger v0 document created with all sections
- [x] Definitions documented (MTTR, Security Debt, Tool Sprawl, Agentic AI)
- [x] Baselines structure defined (with TBD placeholders for data collection)
- [x] Assumptions documented with validation methods
- [x] Decisions Log created with Week 0 decisions
- [x] Locked Parameters defined (scope, LLM model, normalization schema)
- [x] Dependencies listed (scanner versions, Ollama, Go, OPA, etc.)
- [x] Placeholders added for telemetry policy and IRB consent

#### ðŸ“‹ Remaining Tasks

**File Organization**
- [x] **Create `research-ledger/` directory** structure âœ… **COMPLETED**
- [x] **Export Research Ledger v0** to `research-ledger/ledger-v0.md` âœ… **COMPLETED**
- [ ] **Maintain ledger as living document** (update as decisions are made)

**Baseline Data Collection (TBD Items)**
- [ ] **Collect pre-AegisCLI scan times** from CI/CD logs (50 repos, last 6 months or next 2 weeks)
  - Average scan duration per repository
  - Tool switch overhead time (via developer surveys)
  - Total pipeline time including security scans
- [ ] **Measure false positive rates** from 50 repos baseline:
  - Stratified sample: 200 findings (50 per severity level)
  - Gold standard: 3 senior security engineers, independent labeling
  - Calculate FP rate: overall, by scanner (semgrep, trivy, checkov), by severity (CRITICAL, HIGH, MEDIUM, LOW)
  - Target baseline: <30% overall FP rate

**Policy & Compliance Documentation**
- [x] **Create 90-day rolling telemetry policy template** (`docs/telemetry-policy-template.md`) âœ… **COMPLETED**
  - Template includes: opt-in consent process, data retention limits, anonymization rules, data types
  - **Next Step:** Customize template for organization and submit to IRB (Week 2)
- [ ] **Submit IRB application** for human subjects research
  - Include interview protocol (champion interviews)
  - Define telemetry opt-in consent process
  - Get IRB approval document ID
  - Update placeholder in ledger with IRB document ID

**Commands/Tools Required:**
```bash
# Create research-ledger directory
mkdir -p research-ledger

# LOC counting tool (for validation)
# Install cloc: https://github.com/AlDanial/cloc
cloc --sum-reports --report_file=loc_report.txt repo1/ repo2/ ...

# Or use tokei (Rust-based, faster)
tokei repo1/ repo2/ ...
```

---

### Module 0.2: Research Scope & Constraint Mapping

#### âœ… Completed Tasks
- [x] Constraints mapping document created
- [x] Language breakdown targets defined (50K+ LOC across 5 languages)
- [x] Scan run quotas calculated (10,000+ scans per year)
- [x] Air-gapped environment specs defined
- [x] 3 critical risks identified with mitigation strategies
- [x] Risk matrix created (Probability Ã— Impact)

#### ðŸ“‹ Remaining Tasks

**File Organization**
- [x] **Export constraints mapping** to `research-ledger/constraints-mapping.md` âœ… **COMPLETED** (exported from phase_0.md)

**Validation & Verification Tasks**

**1. LOC Count Validation (Week 1)**
- [x] **Create LOC validation script** (`scripts/validate_loc_counts.py`) âœ… **COMPLETED**
- [ ] **Run LOC count** on 50 baseline repositories using `cloc` or `tokei` (requires repo access)
- [ ] **Verify â‰¥50K LOC total** across all repos
- [ ] **Verify language breakdown targets:**
  - Node.js/TypeScript: â‰¥15,000 LOC
  - Python: â‰¥12,000 LOC
  - Go: â‰¥10,000 LOC
  - Java: â‰¥8,000 LOC
  - Terraform/IaC: â‰¥5,000 LOC
- [ ] **Document actual LOC counts** in constraints-mapping.md
- **Script Usage:** `python scripts/validate_loc_counts.py --repos-dir /path/to/repos --output loc_report.json`

**2. Scanner Version Pinning (Week 1)**
- [x] **Create scanner validation script** (`scripts/validate_scanner_versions.py`) âœ… **COMPLETED**
- [ ] **Install and pin scanner versions:**
  - semgrep: v1.45.0+
  - trivy: v0.48.0+
  - checkov: v3.0.0+
  - gitleaks: v8.18.0+
- [ ] **Test SARIF output compatibility** for each scanner version (script automates this)
- [ ] **Document version pinning** in research ledger
- [ ] **Set up quarterly compatibility test schedule** (test new versions, update adapters if needed)
- **Script Usage:** `python scripts/validate_scanner_versions.py --output scanner_validation.json`

**3. GPU Availability Validation (Week 1-2)**
- [x] **Create GPU validation script** (`scripts/validate_gpu.py`) âœ… **COMPLETED**
- [ ] **Identify target air-gap systems** for deployment
- [ ] **Check GPU availability** on air-gap systems (script automates detection):
  - GPU model and VRAM (need 8GB+ for CodeLlama 13B)
  - CUDA/ROCm support
  - Driver compatibility
- [ ] **Test CodeLlama 13B inference speed** on available GPUs (<5s per finding target) (script automates test)
- [ ] **Document GPU validation results** in research ledger
- [ ] **Make fallback decision** by Week 3 if GPU unavailable (CodeLlama 7B or batch processing)
- **Script Usage:** `python scripts/validate_gpu.py --output gpu_validation.json`

**4. Ollama Model Setup (Week 1)**
- [ ] **Install Ollama** v0.1.15+ on target systems
- [ ] **Download CodeLlama 13B model:** `ollama pull codellama:13b`
- [ ] **Verify model size** (~7GB for Q4_K_M quantization)
- [ ] **Test model inference** with sample prompts
- [ ] **Pre-download models** for air-gap deployment (USB transfer preparation)

**5. Air-Gap Deployment Test (Week 2)**
- [ ] **Set up isolated Ubuntu 22.04 VM** (network disabled)
- [ ] **Install AegisCLI** (static Go binaries + Ollama binary)
- [ ] **Transfer Ollama models** via USB/internal network
- [ ] **Verify full functionality** in air-gap mode:
  - Scanner execution (semgrep, trivy, checkov)
  - SARIF normalization
  - LLM triage (CodeLlama via Ollama)
  - Policy evaluation (OPA/Rego)
  - Dashboard (PostgreSQL + Grafana)
- [ ] **Document deployment test results** (success/failure, issues encountered)

**6. Scan Run Monitoring Setup (Week 1)**
- [ ] **Identify 20 teams** for scan run tracking (5 high-activity, 10 medium, 5 low-activity)
- [ ] **Set up CI/CD log monitoring** (GitLab/Jenkins/GitHub Actions integration)
- [ ] **Implement scan invocation counter** in AegisCLI telemetry
- [ ] **Verify scan quotas** are being met (440 scans/week target)

**7. Champion Identification (Week 1-4)**
- [ ] **Identify primary security champions** for each of 20 teams
- [ ] **Identify backup champions** (2 per team) to mitigate attrition risk
- [ ] **Document champion identification criteria** in research ledger
- [ ] **Set up monthly attrition monitoring** (track champion presence/engagement)

**Commands/Tools Required:**
```bash
# LOC counting (cloc)
cloc --by-file --report-file=loc_report.csv repo1/ repo2/ ... repo50/

# LOC counting (tokei - Rust-based, faster)
tokei --output json repo1/ repo2/ ... repo50/ > loc_report.json

# Scanner version verification
semgrep --version  # Should be â‰¥1.45.0
trivy --version    # Should be â‰¥0.48.0
checkov --version  # Should be â‰¥3.0.0
gitleaks --version # Should be â‰¥8.18.0

# Ollama model download
ollama pull codellama:13b

# GPU check (Linux)
nvidia-smi  # For NVIDIA GPUs
lspci | grep -i vga  # For GPU hardware check

# Docker test for air-gap deployment (if using containers)
docker save aegiscli:latest | gzip > aegiscli-airgap.tar.gz
# Transfer to air-gap system
docker load < aegiscli-airgap.tar.gz
```

---

## TODO SUMMARY BY PRIORITY

### Priority 1 (Critical - Blocks Phase 1-2 Progress)
1. âœ… **Create research-ledger directory** and export ledger files - **COMPLETED**
2. **GPU availability validation** (Week 1-2) - Critical for LLM triage feature (script ready)
3. **Scanner version pinning** (Week 1) - Required for reproducibility (script ready)
4. **Ollama model setup** (Week 1) - Required for local LLM functionality

### Priority 2 (High Value - Strengthens Foundation)
5. **LOC count validation** (Week 1) - Validates 50K+ LOC constraint (script ready)
6. **Air-gap deployment test** (Week 2) - Validates technical feasibility
7. âœ… **Telemetry policy template creation** (Week 2) - **COMPLETED** (template ready, needs customization)
8. **IRB application submission** (Week 2-3) - Required for human subjects research

### Priority 3 (Ongoing - Supports Long-term Success)
9. **Baseline data collection** (Ongoing, starts Week 1) - FP rates, scan times
10. **Champion identification** (Week 1-4) - Supports RQ4
11. **Scan run monitoring setup** (Week 1) - Tracks 10,000+ scan constraint

---

## ESTIMATED TIMELINE

| Task Category | Estimated Hours | Responsible | Deadline |
|--------------|----------------|-------------|----------|
| Directory structure setup | 1 hour | Research Lead | Week 1, Day 1 |
| LOC count validation | 4-6 hours | Research Team | Week 1, Day 2-3 |
| Scanner version pinning & testing | 8-12 hours | Engineering Lead | Week 1, Day 3-5 |
| GPU validation | 4-8 hours | Engineering Lead | Week 2, Day 1-2 |
| Ollama model setup | 2-4 hours | Engineering Lead | Week 1, Day 5 |
| Air-gap deployment test | 8-16 hours | Engineering Team | Week 2, Day 3-5 |
| Telemetry policy creation | 4-6 hours | Research Lead | Week 2 |
| IRB application | 8-16 hours | Research Lead | Week 2-3 (pending review) |
| Baseline data collection | 40-80 hours | Research Team | Weeks 1-4 (ongoing) |
| Champion identification | 8-16 hours | Research Lead | Week 1-4 |
| **Total Estimated** | **87-165 hours** | **2-4 weeks** | **End of Week 4** |

---

## VALIDATION CHECKLIST

### Week 1 Checklist
- [ ] Research ledger directory created and files exported
- [ ] LOC count validation: â‰¥50K LOC confirmed across 50 repos
- [ ] Scanner versions pinned and SARIF compatibility tested
- [ ] Ollama installed and CodeLlama 13B model downloaded
- [ ] GPU availability checked on air-gap systems

### Week 2 Checklist
- [ ] GPU validation complete (inference speed <5s per finding OR fallback decision made)
- [ ] Air-gap deployment test: All components functional in isolated environment
- [ ] Telemetry policy document created (`docs/telemetry-policy.md`)
- [ ] IRB application submitted (or in progress)

### Week 4 Checklist
- [ ] Champion identification complete (primary + backup champions for 20 teams)
- [ ] Baseline data collection started (FP rate measurement in progress)
- [ ] Scan run monitoring active (tracking 440 scans/week target)
- [ ] All Priority 1 and Priority 2 tasks complete

---

## NOTES

- **Dependencies:** Many tasks depend on infrastructure access (air-gap systems, GPU availability, CI/CD logs)
- **Parallel Execution:** GPU validation, scanner pinning, and Ollama setup can be done in parallel (Week 1)
- **Iterative Updates:** Research ledger should be updated as decisions are made and validations completed
- **Risk Monitoring:** GPU availability (R1) is highest priority; validate early to trigger fallback plans if needed
- **Quality Gates:** Each validation task must pass before proceeding to Phase 1 (Idea Refinement)

---

## EXECUTION STATUS

**Last Updated:** Phase 0 Execution  
**Current Status:** Documentation Complete, Validation Tasks Pending  
**Next Steps:** Begin Priority 1 validation tasks (Week 1)

---

## TODO EXECUTION RESULTS

This section documents the execution of automated/structural tasks from the TODO list.

### âœ… Completed Automated Tasks

#### 1. Directory Structure Creation
**Status:** âœ… Complete  
**Date:** Execution Run

**Created Directories:**
- `research-ledger/` (root directory for ledger files)
- `docs/` (for telemetry policy and other documentation)

**Files Created:**
- `research-ledger/README.md` - Documentation for research ledger structure and version history

**Usage:**
- Research Ledger files (ledger-v0.md, ledger-v1.md, etc.) should be stored in this directory
- Constraint mapping file should be moved from phase_0.md to `research-ledger/constraints-mapping.md`

---

### ðŸ“‹ Pending Tasks (Require Manual Execution/Infrastructure Access)

**These tasks require actual execution, infrastructure access, or research work:**

#### Module 0.1 - File Organization
- [x] **Export Research Ledger v0** from phase_0.md to `research-ledger/ledger-v0.md` âœ… **COMPLETED**
- [x] **Export Constraints Mapping** from phase_0.md to `research-ledger/constraints-mapping.md` âœ… **COMPLETED**
- [ ] Maintain ledger as living document (update as Phase 1-7 progress)

#### Module 0.1 - Baseline Data Collection
- [ ] Collect pre-AegisCLI scan times (requires CI/CD log access)
- [ ] Measure false positive rates (requires paper review by 3 security engineers)

#### Module 0.1 - Policy & Compliance
- [ ] Create `docs/telemetry-policy.md` (requires policy definition)
- [ ] Submit IRB application (requires institutional IRB process)

#### Module 0.2 - Validation Tasks
- [ ] LOC count validation (requires access to 50 repositories)
- [ ] Scanner version pinning & testing (requires tool installation)
- [ ] GPU availability validation (requires air-gap system access)
- [ ] Ollama model setup (requires installation and model download)
- [ ] Air-gap deployment test (requires isolated VM setup)
- [ ] Champion identification (requires organizational chart access)

---

### ðŸ”§ Command Reference

**Useful commands for validation tasks (when infrastructure is available):**

```bash
# LOC counting (cloc)
cloc --sum-reports --report_file=loc_report.txt repo1/ repo2/ ... repo50/

# LOC counting (tokei - Rust-based, faster)
tokei --output json repo1/ repo2/ ... repo50/ > loc_report.json

# Scanner version verification
semgrep --version   # Should be â‰¥1.45.0
trivy --version     # Should be â‰¥0.48.0
checkov --version   # Should be â‰¥3.0.0
gitleaks --version  # Should be â‰¥8.18.0

# Ollama model download
ollama pull codellama:13b

# GPU check (Linux)
nvidia-smi          # For NVIDIA GPUs
lspci | grep -i vga # For GPU hardware check

# Go version check
go version          # Should be â‰¥1.21
```

---

### ðŸ“Š Execution Summary

| Category | Tasks Completed | Tasks Pending | Notes |
|----------|----------------|---------------|-------|
| **Directory Structure** | âœ… 1/1 | 0 | Directories created |
| **Documentation** | âœ… 1/1 | 0 | README created |
| **File Organization** | âœ… 2/2 | 0 | Research Ledger v0 and Constraints Mapping exported to research-ledger/ |
| **Baseline Data Collection** | 0 | 2 | Requires CI/CD access + security engineer review |
| **Policy & Compliance** | 0 | 2 | Requires policy writing + IRB submission |
| **Validation Tasks** | 0 | 6 | Requires infrastructure access (repos, systems, tools) |
| **Champion Identification** | 0 | 1 | Requires organizational access |

**Automation Coverage:** ~15% of tasks can be automated (structural tasks)  
**Manual/Infrastructure Work Required:** ~85% of tasks require access to systems, repositories, or manual research work

---

**Execution Status:** âœ… Structural foundation complete (directories, documentation, file exports, validation scripts)  
**Pending:** All validation and data collection tasks (require infrastructure/research access)

---

## ADDITIONAL EXECUTION RESULTS - Validation Scripts & Templates

### âœ… Completed Automation & Templates

#### 1. Validation Scripts Created
**Status:** âœ… Complete  
**Date:** Execution Run  
**Location:** `scripts/` directory

**Script 1: `validate_loc_counts.py`**
- **Purpose:** Validate LOC counts across 50 repositories
- **Features:**
  - Supports both `cloc` and `tokei` tools
  - Categorizes languages into target ecosystems
  - Validates â‰¥50K LOC total and per-language targets
  - Generates JSON report with validation results
- **Usage:** `python scripts/validate_loc_counts.py --repos-dir /path/to/repos --output loc_report.json --tool cloc`
- **Output:** JSON report with summary, language breakdown, per-repo details

**Script 2: `validate_scanner_versions.py`**
- **Purpose:** Validate scanner versions meet requirements and test SARIF output
- **Features:**
  - Checks version for semgrep, trivy, checkov, gitleaks
  - Tests SARIF output compatibility for each scanner
  - Generates validation report
- **Usage:** `python scripts/validate_scanner_versions.py --output scanner_validation.json`
- **Output:** JSON report with version status and SARIF compatibility for each scanner

**Script 3: `validate_gpu.py`**
- **Purpose:** Validate GPU availability and CodeLlama 13B inference speed
- **Features:**
  - Detects NVIDIA GPUs via nvidia-smi
  - Checks GPU memory (8GB+ requirement)
  - Tests Ollama CodeLlama 13B inference speed (<5s target)
  - Provides fallback recommendations if GPU unavailable
- **Usage:** `python scripts/validate_gpu.py --output gpu_validation.json`
- **Output:** JSON report with GPU details, inference test results, overall status

#### 2. Telemetry Policy Template Created
**Status:** âœ… Complete  
**Date:** Execution Run  
**Location:** `docs/telemetry-policy-template.md`

**Content:**
- Complete 90-day rolling telemetry policy template
- All required sections: Purpose, Scope, Opt-In Consent, Data Types, Retention, Anonymization
- GDPR compliance considerations
- Data deletion and right to erasure procedures
- Ready for customization and IRB review

**Next Steps:**
- Customize template for organization-specific requirements (Week 2)
- Submit to IRB for review
- Finalize as `docs/telemetry-policy.md` after approval

#### 3. Validation Checklist Created
**Status:** âœ… Complete  
**Date:** Execution Run  
**Location:** `scripts/phase0_validation_checklist.md`

**Content:**
- Week-by-week validation checklist
- Task breakdown by day/week
- Quality gates before Phase 1
- Usage instructions for validation scripts
- Notes and reminders

**Usage:**
- Track progress on validation tasks
- Ensure all quality gates met before Phase 1
- Reference for team coordination

---

### ðŸ“Š Updated Execution Summary

| Category | Tasks Completed | Tasks Pending | Notes |
|----------|----------------|---------------|-------|
| **Directory Structure** | âœ… 1/1 | 0 | Directories created |
| **Documentation** | âœ… 1/1 | 0 | README created |
| **File Organization** | âœ… 2/2 | 0 | Research Ledger v0 and Constraints Mapping exported |
| **Validation Scripts** | âœ… 3/3 | 0 | LOC, Scanner, GPU validation scripts created |
| **Templates** | âœ… 2/2 | 0 | Telemetry policy template + validation checklist |
| **Baseline Data Collection** | 0 | 2 | Requires CI/CD access + security engineer review |
| **Policy & Compliance** | 0 | 2 | Requires policy customization + IRB submission |
| **Validation Tasks** | 0 | 6 | Requires infrastructure access (repos, systems, tools) |
| **Champion Identification** | 0 | 1 | Requires organizational access |

**Automation Coverage:** ~40% of tasks can be automated (structural tasks, scripts, templates)  
**Manual/Infrastructure Work Required:** ~60% of tasks require access to systems, repositories, or manual research work

---

### ðŸ”§ Script Usage Quick Reference

**All scripts require Python 3.7+ and are located in `scripts/` directory:**

```bash
# LOC Count Validation
python scripts/validate_loc_counts.py \
  --repos-dir /path/to/50/repos \
  --output loc_report.json \
  --tool cloc

# Scanner Version Validation
python scripts/validate_scanner_versions.py \
  --output scanner_validation.json

# GPU Validation
python scripts/validate_gpu.py \
  --output gpu_validation.json
```

**All scripts:**
- Output JSON reports for documentation
- Return exit code 0 on success, 1 on failure
- Print summary to console
- Can be integrated into CI/CD pipelines

---

### ðŸ“ Complete File Structure

```
SecureCLI Agent /
â”œâ”€â”€ research-ledger/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ ledger-v0.md          âœ… Exported
â”‚   â””â”€â”€ constraints-mapping.md âœ… Exported
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ telemetry-policy-template.md âœ… Created
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate_loc_counts.py        âœ… Created
â”‚   â”œâ”€â”€ validate_scanner_versions.py  âœ… Created
â”‚   â”œâ”€â”€ validate_gpu.py                âœ… Created
â”‚   â””â”€â”€ phase0_validation_checklist.md âœ… Created
â””â”€â”€ phase_0.md (updated with all execution results)
```

---

**Final Execution Status:** âœ… All executable tasks complete  
**Ready For:** Infrastructure access to run validation scripts and collect baseline data

---

## PHASE 0 EXECUTION SUMMARY

### âœ… Completed Tasks (All Executable Items)

| Task Category | Items Completed | Status |
|--------------|----------------|--------|
| **Directory Structure** | research-ledger/, docs/ | âœ… Complete |
| **File Exports** | ledger-v0.md, constraints-mapping.md | âœ… Complete |
| **Validation Scripts** | 3 scripts (LOC, Scanner, GPU) | âœ… Complete |
| **Templates** | Telemetry policy template, Validation checklist | âœ… Complete |
| **Documentation** | README files, execution results | âœ… Complete |

### ðŸ“‹ Remaining Tasks (Require Infrastructure/Research Access)

| Task Category | Items Pending | Blocking Factor |
|--------------|---------------|-----------------|
| **LOC Validation** | Run script on 50 repos | Repository access required |
| **Scanner Validation** | Install & test scanners | Tool installation required |
| **GPU Validation** | Run script on air-gap systems | System access required |
| **Ollama Setup** | Download model, test inference | System access + model download |
| **Air-Gap Test** | Deploy & test in isolated VM | VM setup + deployment |
| **Baseline Data** | Collect scan times, FP rates | CI/CD access + security engineers |
| **Telemetry Policy** | Customize template, IRB review | Policy customization + IRB process |
| **IRB Application** | Submit & get approval | IRB submission process |
| **Champion ID** | Identify champions for 20 teams | Organizational access |

### ðŸŽ¯ Automation Achievements

**Before Execution:**
- 0% automation (all tasks manual)

**After Execution:**
- **40% automation coverage** (structural tasks, scripts, templates)
- **3 validation scripts** ready for infrastructure access
- **2 templates** ready for customization
- **Complete documentation** for all tasks

### ðŸ“ˆ Progress Metrics

- **Structural Tasks:** 100% complete (6/6)
- **Automation Scripts:** 100% complete (3/3)
- **Templates:** 100% complete (2/2)
- **Infrastructure Tasks:** 0% complete (0/9) - awaiting access
- **Overall Phase 0:** ~40% complete (executable portion)

### ðŸš€ Next Steps

1. **Week 1:** Gain infrastructure access â†’ Run validation scripts
2. **Week 2:** Customize telemetry policy â†’ Submit IRB application
3. **Week 3-4:** Complete baseline data collection â†’ Champion identification

**All automation and structural work is complete. Phase 0 is ready for infrastructure-dependent validation tasks.**

---

## EXECUTION RESULTS - File Organization Tasks

### âœ… Completed File Exports

**Date:** Execution Run

**1. Research Ledger v0 Export**
- **Source:** `phase_0.md` (lines 5-161)
- **Destination:** `research-ledger/ledger-v0.md`
- **Status:** âœ… Exported successfully
- **Content:** All 7 sections (Definitions, Baselines, Assumptions, Decisions Log, Locked Parameters, Dependencies, Placeholders)

**2. Constraints Mapping Export**
- **Source:** `phase_0.md` (lines 164-349)
- **Destination:** `research-ledger/constraints-mapping.md`
- **Status:** âœ… Exported successfully
- **Content:** All 6 sections (Code Size Constraints, Scan Run Targets, Air-Gap Specs, Critical Risks, Risk Matrix, Additional Constraints)

**File Structure Created:**
```
research-ledger/
â”œâ”€â”€ README.md
â”œâ”€â”€ ledger-v0.md          âœ… Exported
â””â”€â”€ constraints-mapping.md âœ… Exported
```

**Next Steps:**
- Ledger files are now separated and ready for updates as Phase 1-7 progress
- `phase_0.md` maintains the consolidated view with module outputs
- Future ledger versions (v1, v2, etc.) will be added to `research-ledger/` as phases complete

