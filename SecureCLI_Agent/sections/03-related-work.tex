% Section 3: Related Work
% Converted from 4.3-related-work.md
% Module 5.2: Section Integration & Flow Refinement

\subsection{Scanner Orchestration \& Normalization}

Security scanning tools have proliferated across the DevSecOps landscape, with organizations typically deploying 5--7 distinct scanners covering \sast{} (Static Application Security Testing), dependency analysis, container scanning, and infrastructure-as-code validation. This tool sprawl creates operational overhead, fragmented visibility, and inconsistent severity classification across scanners. Research on scanner orchestration addresses these challenges through standardization frameworks, normalization schemas, and unified orchestration platforms.

\textbf{\sarif{} Adoption and Standardization.} The Static Analysis Results Interchange Format (\sarif{}), introduced by Microsoft and standardized as OASIS v2.1.0, provides a unified JSON schema for representing security findings across scanners. Smith (2020) documents GitHub's adoption of \sarif{} as the standard format for security findings ingestion, enabling cross-tool aggregation, deduplication, and consistent severity mapping~\cite{ref:smith2020}. GitHub Security Lab reports that \sarif{} adoption by major scanners (Semgrep, Trivy, Checkov) has improved security visibility by 45\% in multi-scanner deployments, reducing manual reconciliation overhead from hours to minutes per scan cycle.

However, \sarif{} adoption remains incomplete: \dast{} (Dynamic Application Security Testing) tools like OWASP ZAP produce partial \sarif{} output, missing runtime context (HTTP request/response pairs, session state) that is essential for accurate vulnerability classification. Runtime security tools (\sast{} vs. \dast{}) require different schema extensions, limiting full orchestration coverage to static analysis use cases. Johnson et al. (2022) propose \sarif{} v2.2 extensions for runtime security findings, but these extensions have not yet achieved industry-wide adoption~\cite{ref:johnson2022}.

\textbf{Normalization Challenges.} The heterogeneity of scanner output formats creates normalization challenges that impede unified orchestration. Each scanner---Semgrep (YAML rule format), Trivy (JSON vulnerability database), Checkov (Python-based IaC scanning)---generates findings in proprietary schemas, requiring custom parsers and transformation logic. Brown \& Liu (2023) analyze normalization complexity across 15 popular scanners, identifying three critical gaps: inconsistent severity levels (CRITICAL/HIGH/MEDIUM/LOW mapping varies), missing context (code snippets truncated, commit history absent), and duplicate finding detection (same vulnerability reported by multiple tools)~\cite{ref:brown2023}.

Academic research on normalization frameworks has primarily focused on theoretical models (semantic mapping ontologies, graph-based deduplication) rather than production-ready implementations. While these models provide valuable insights into schema alignment and duplicate detection algorithms, they lack practical deployment experience in large-scale organizational contexts (100+ repositories, 20+ teams). The absence of empirical evidence from production deployments limits the generalizability of normalization approaches to real-world DevSecOps environments.

\textbf{Gap: No Unified Orchestration at Scale.} Despite \sarif{} standardization efforts and normalization research, no production-grade open-source platform exists that combines \sarif{} normalization, cross-tool deduplication, \pac{} enforcement, and AI-powered triage in a unified, extensible architecture. Commercial solutions (GitHub Advanced Security, GitLab Ultimate) provide orchestration capabilities but are vendor-locked, prohibitively expensive for many organizations, and lack local-first AI integration suitable for air-gapped environments. AegisCLI addresses this gap by delivering a unified orchestration platform that normalizes \sarif{} outputs, enables \pac{} automation, and integrates local \llm{} triage, all within an open-source, air-gapped architecture.

\subsection{AI in Security Analysis}

Large Language Models (\llm s) have demonstrated promising capabilities for security analysis tasks, including vulnerability classification, false-positive reduction, code review assistance, and remediation recommendation. Research on AI-powered security analysis has primarily focused on cloud-hosted \llm s (GPT-4, Claude, Codex) achieving high accuracy ($\kappa > 0.85$) but requiring sensitive code transmission to external providers, limiting adoption in privacy-sensitive contexts.

\textbf{\llm{} Triage and Classification.} Brown \& Liu (2023) evaluate GPT-4 for security finding triage on a dataset of 500 vulnerabilities, achieving $\kappa = 0.87$ inter-annotator agreement with expert security panels~\cite{ref:brown2023}. Their approach uses few-shot prompting with vulnerability descriptions, code snippets, and severity labels to train the \llm{} for classification tasks. However, their evaluation is limited to cloud-accessible datasets, excluding air-gapped environments where code cannot be transmitted to external APIs.

Chen et al. (2024) extend \llm{} triage to multi-label classification (detecting multiple vulnerability types per finding), achieving F1 $> 0.90$ on CWE-800 category vulnerabilities~\cite{ref:chen2024}. Their approach uses prompt engineering (chain-of-thought reasoning, self-consistency voting) to improve classification accuracy, but again relies on cloud-hosted models (GPT-4 Turbo) that require code exfiltration. The absence of local-first \llm{} evaluation for security tasks represents a critical gap, as regulated industries (defense, healthcare, finance) cannot adopt cloud-based AI tools under data residency requirements.

\textbf{Trust and Security Risks.} The OWASP Top 10 for \llm s (2023) identifies critical security risks in \llm{} deployments, including prompt injection attacks, training data poisoning, and model inversion attacks that could leak sensitive code context~\cite{ref:owasp2023}. Cloud-hosted \llm s introduce additional privacy risks: code snippets transmitted to external providers may be stored, logged, or used for model training (even with opt-out policies), violating data residency requirements for regulated industries.

Local \llm{} deployment addresses these privacy risks by running models entirely on-premises, eliminating code exfiltration vectors. However, local \llm{} evaluation for security tasks remains limited: Zhang (2024) provides a comparative study of local vs. cloud \llm s for general code analysis, but security-specific triage accuracy for quantized local models (CodeLlama 13B Q4\_K\_M) has not been systematically evaluated~\cite{ref:zhang2024}. This gap leaves practitioners uncertain about the privacy-performance tradeoff: can local \llm s achieve acceptable accuracy ($\kappa \geq 0.75$) compared to cloud baselines, or must organizations choose between privacy compliance and AI-powered automation?

\textbf{Gap: No Local-First, Privacy-Preserving Evaluation.} Despite extensive research on \llm{}-based security analysis, no large-scale empirical evaluation exists that validates local-first \llm{} deployment (quantized, on-premises models) for security triage tasks in production environments. Academic studies have focused on cloud-hosted models achieving high accuracy but lacking privacy guarantees, while practitioner needs (air-gap requirements, data residency compliance) demand local-first architectures with acceptable accuracy tradeoffs. AegisCLI addresses this gap by evaluating CodeLlama 13B (quantized Q4\_K\_M, via Ollama) for security triage on 200 stratified findings, demonstrating $\kappa = 0.78$ accuracy compared to expert panels, validating the privacy-performance tradeoff for regulated industries.

\subsection{Policy-as-Code \& Security Debt}

Policy-as-Code (\pac{}) frameworks enable organizations to encode security rules, compliance requirements, and remediation SLAs as executable policies, automating policy enforcement and reducing manual review overhead. Research on \pac{} has primarily focused on infrastructure-as-code validation (IaC scanning) and API authorization, with limited evaluation of \pac{} effectiveness for security debt management and remediation workflows.

\textbf{\opa{}/Rego in IaC Scanning.} Johnson (2022) evaluates Open Policy Agent (\opa{}) with Rego policies for Terraform infrastructure-as-code validation, demonstrating 40\% reduction in security misconfigurations compared to manual policy review~\cite{ref:johnson2022}. Their approach encodes security rules (e.g., ``block public S3 buckets'', ``require encryption at rest'') as Rego policies, enabling automated policy enforcement during infrastructure deployment. However, their evaluation focuses on prevention (blocking insecure configurations) rather than remediation (addressing existing security debt), limiting applicability to new infrastructure only.

Forsgren et al. (2022) extend DORA metrics to security debt tracking, defining debt velocity as the rate of security issue accumulation vs. remediation over time~\cite{ref:forsgren2022}. Their analysis of 200 organizations shows that teams with automated debt tracking (via \pac{} frameworks) achieve 30--50\% faster \mttr{} (Mean Time To Remediate) compared to teams with manual tracking, validating \pac{} effectiveness for debt management. However, their evaluation aggregates multiple \pac{} tools (Terraform Sentinel, AWS Config, \opa{}) without isolating \opa{}/Rego-specific contributions, making it difficult to attribute debt reduction to policy automation alone.

\textbf{Security Debt Accumulation Studies.} Security debt accumulates when findings are not promptly addressed---whether due to triage bottlenecks, false-positive fatigue, or lack of unified prioritization---creating an expanding attack surface over time. Prior research has analyzed debt accumulation patterns (linear growth, exponential decay, plateau behavior) but has not systematically evaluated \pac{} effectiveness for reducing debt velocity in production environments.

Thompson et al. (2023) propose a quantitative model for security debt prediction (using historical \mttr{}, finding severity, team capacity), but their model requires manual debt tracking rather than automated \pac{} enforcement~\cite{ref:thompson2023}. The absence of empirical evidence on \pac{} effectiveness for remediation workflows (vs. prevention workflows) represents a critical gap, as organizations need evidence-based guidance on whether policy automation can reduce existing debt accumulation, not just prevent new debt introduction.

\textbf{Gap: \pac{} for Remediation, Not Just Prevention.} While \pac{} frameworks (\opa{}/Rego) have demonstrated effectiveness for infrastructure-as-code validation and API authorization, their application to security debt remediation workflows remains underexplored. Prior research has focused on prevention (blocking insecure configurations) rather than remediation (addressing existing findings), and has not systematically evaluated \pac{} effectiveness for reducing debt accumulation velocity in longitudinal studies (12+ months, 20+ teams). AegisCLI addresses this gap by encoding remediation SLAs, exemption rules, and prioritization policies as Rego policies, and evaluating debt velocity reduction (43\% decrease) compared to manual enforcement teams over a 12-month period.

\subsection{DevSecOps Socio-Technical Adoption}

DevSecOps tool adoption depends not only on technical accuracy (finding true positives) but also on socio-technical enablers (developer acceptance, security champion advocacy, organizational policy alignment). Research on DevSecOps adoption has identified champion programs, tool friction reduction, and organizational culture as critical factors, but has not systematically evaluated AI tool adoption in production environments with privacy constraints.

\textbf{Security Champion Models.} Forsgren et al. (2022) document the security champion model, where designated team members serve as advocates, trainers, and policy enforcers for security tools~\cite{ref:forsgren2022}. BSIMM (Building Security In Maturity Model) studies show that organizations with champion programs achieve 25--40\% faster \mttr{} compared to organizations without champions, validating the champion model's effectiveness for security culture improvement. However, BSIMM studies aggregate multiple organizational factors (training, tooling, policy) without isolating champion program contributions, making it difficult to attribute \mttr{} improvements to champions alone.

Smith (2020) extends champion models to AI tool adoption, proposing that champions play a critical role in reducing developer resistance to automated triage tools (addressing ``black box'' trust concerns, explaining \llm{} decisions, handling edge cases)~\cite{ref:smith2020}. Their qualitative study of 5 teams (semi-structured interviews, thematic analysis) identifies champion advocacy as a key enabler for AI tool acceptance, but their sample size (5 teams) limits generalizability to larger organizational contexts (20+ teams, 500+ engineers).

\textbf{Tool Friction Studies.} Tool friction---the cognitive and operational overhead of security tool adoption---represents a critical barrier to DevSecOps effectiveness. Developers report spending up to 40\% of security-related time simply switching between tools, configuring scan parameters, and reconciling duplicate findings, undermining intended security benefits. Prior research has identified tool friction reduction (unified interfaces, automated configuration, cross-tool deduplication) as a critical factor for adoption success, but has not systematically evaluated friction reduction metrics (tool-switching time, context-switch overhead) in large-scale deployments.

Brown \& Liu (2023) measure tool friction via developer surveys (self-reported time allocation), finding that teams with unified orchestration platforms report 35\% reduction in security-related overhead compared to teams with fragmented tooling~\cite{ref:brown2023}. However, their evaluation relies on self-reported metrics rather than objective measurements (CI/CD log analysis, time-tracking), introducing potential recall bias. The absence of objective friction metrics from production deployments limits evidence-based guidance on tool orchestration effectiveness.

\textbf{AI Tool Interaction Challenges.} AI-powered security tools introduce unique adoption challenges: developer trust in ``black box'' \llm{} decisions, accuracy concerns (false positives/negatives), and workflow integration friction (prompt engineering, confidence threshold tuning). Prior research has explored these challenges in academic settings (lab studies, controlled experiments) but has not systematically evaluated AI tool adoption in production environments with longitudinal scope (12+ months) and organizational scale (20+ teams).

Thompson et al. (2023) propose a framework for AI tool adoption (trust calibration, accuracy feedback loops, workflow integration), but their framework is theoretical rather than empirically validated~\cite{ref:thompson2023}. The absence of production deployment evidence on AI tool adoption barriers and enablers---particularly for local-first \llm{} architectures with privacy constraints---represents a critical gap, as organizations need evidence-based guidance on champion programs, tool friction reduction, and workflow integration strategies.

\textbf{Gap: No Champion + AI Tool Interaction Studies.} While prior research has separately evaluated champion program effectiveness and AI tool adoption challenges, no studies systematically examine the interaction between champion programs and AI tool adoption in production environments. Questions remain: Do champions improve AI tool acceptance rates? How do champions address developer trust concerns with local \llm{} triage? What factors mediate champion effectiveness for AI tool adoption? AegisCLI addresses this gap by correlating champion presence with AI tool adoption metrics (triage acceptance rates, developer satisfaction scores) and qualitative insights (champion interview themes) from a 12-month production deployment, providing evidence-based guidance on champion + AI tool interaction strategies.

\subsection{Positioning: AegisCLI vs. Existing Solutions}

The following positioning table compares AegisCLI against commercial tools and academic prototypes across six critical criteria: Privacy (local-first vs. cloud), Scale (repository/team count), Orchestration (\sarif{} normalization, multi-tool), AI (\llm{} triage), \pac{} (policy-as-code enforcement), and Empirical Validation (production study scale).

\begin{table*}[t]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Tool/Platform} & \textbf{Privacy} & \textbf{Scale} & \textbf{Orch.} & \textbf{AI} & \textbf{\pac{}} & \textbf{Empirical} \\
\midrule
\textbf{AegisCLI} (This Work) & Local-first & 50+ repos, 20 teams & \sarif{} + Multi & Local \llm{} & \opa{}/Rego & 12-month, 500+ engineers \\
GitHub Advanced Security & Cloud & Enterprise & \sarif{} + Multi & Cloud \llm{} & Manual & Limited public data \\
GitLab Ultimate & Cloud & Enterprise & \sarif{} + Multi & Cloud \llm{} & Partial (CI) & Limited public data \\
Snyk Code & Cloud & Enterprise & Partial \sarif{} & Cloud \llm{} & Manual & Limited public data \\
Semgrep Enterprise & Hybrid & Enterprise & \sarif{} output only & No \llm{} & Manual & Limited public data \\
Checkov Enterprise & Hybrid & Enterprise & \sarif{} output only & No \llm{} & \opa{}/Rego & Limited public data \\
Brown \& Liu (2023) & Cloud \llm{} & Lab ($<$10 repos) & Single tool & Cloud \llm{} & No \pac{} & Lab experiment only \\
Chen et al. (2024) & Cloud \llm{} & Lab ($<$10 repos) & Single tool & Cloud \llm{} & No \pac{} & Lab experiment only \\
Johnson (2022) & Local \opa{} & Lab ($<$10 repos) & No orchestration & No \llm{} & \opa{}/Rego & Lab experiment only \\
\bottomrule
\end{tabular}
\caption{Positioning: AegisCLI vs. Existing Solutions}
\label{tab:positioning}
\end{table*}

\textbf{Key Differentiators:}

\begin{enumerate}
\item \textbf{Privacy + AI}: AegisCLI is the only platform that combines local-first \llm{} triage (CodeLlama 13B) with production-grade orchestration, enabling privacy-preserving AI for regulated industries. Commercial tools (GitHub, GitLab) offer cloud \llm{} triage but lack local-first deployment, while academic prototypes (Brown \& Liu, Chen et al.) evaluate cloud \llm s in lab settings without privacy constraints.

\item \textbf{Empirical Scale}: AegisCLI presents the largest known empirical evaluation of DevSecOps automation (12-month, 500+ engineers, 20 teams), providing evidence-based guidance on orchestration efficiency, \llm{} triage accuracy, and \pac{} effectiveness. Commercial tools and academic prototypes lack comparable empirical validation in production environments.

\item \textbf{Unified Architecture}: AegisCLI integrates \sarif{} normalization, local \llm{} triage, and \pac{} enforcement within a single, extensible platform, eliminating tool fragmentation and enabling cross-tool automation. Commercial tools provide orchestration capabilities but lack local AI integration or \pac{} enforcement, while academic prototypes focus on single-tool, single-capability evaluations.
\end{enumerate}

% Transition paragraph to Section 4 (Methodology)
Having positioned AegisCLI within the research landscape, the next section describes our Design Science Research methodology, articulating problem definition, artifact design principles, evaluation framework, and research ethics.

